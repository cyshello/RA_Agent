#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
JSON íŒŒì¼ì„ MariaDB/MySQLì— ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (Schema-first, ì„ë² ë”© í¬í•¨)

ì‚¬ìš©ë²•:
    python load_json_to_db.py [options]

ì˜ˆì‹œ:
    python load_json_to_db.py
    python load_json_to_db.py --type project
    python load_json_to_db.py --type management_eval
    python load_json_to_db.py --type inclusive_growth
    python load_json_to_db.py --reset
"""

import argparse
import json
import os
import sys
import struct
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple

import pymysql
import dotenv
from openai import OpenAI


# =========================================================
# ê¸°ë³¸ ì„¤ì •
# =========================================================
DEFAULT_DB_HOST = "localhost"
DEFAULT_DB_PORT = 3306
DEFAULT_DB_NAME = "b2g_data"
DEFAULT_DB_USER = "root"
DEFAULT_DB_PASSWORD = ""

EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIM = 1536  # í˜„ì¬ ê²€ì¦/ì°¸ê³ ìš©

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# OpenAI API í‚¤ ë¡œë“œ
env_path = os.path.join(SCRIPT_DIR, ".env")
if os.path.exists(env_path):
    OPENAI_KEY = dotenv.get_key(env_path, "OPENAI_KEY")
else:
    OPENAI_KEY = os.environ.get("OPENAI_KEY")

openai_client = None


# =========================================================
# Schema Registry (ë‹¨ì¼ ì§„ì‹¤ì›ì²œ)
# =========================================================
from src.db_main import SCHEMA_REGISTRY


# =========================================================
# ê³µí†µ ìœ í‹¸
# =========================================================
def get_schema(data_type: str) -> Dict[str, Any]:
    if data_type not in SCHEMA_REGISTRY:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {data_type}")
    return SCHEMA_REGISTRY[data_type]


def get_connection(host: str, port: int, database: str, user: str, password: str):
    return pymysql.connect(
        host=host,
        port=port,
        database=database,
        user=user,
        password=password,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
    )


def init_openai_client():
    global openai_client
    if openai_client is None:
        if not OPENAI_KEY:
            print("âš ï¸ OPENAI_KEY ì—†ìŒ. ì„ë² ë”© ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
            return None
        openai_client = OpenAI(api_key=OPENAI_KEY)
    return openai_client


def get_embeddings_batch(texts: List[str], batch_size: int = 100) -> List[Optional[bytes]]:
    client = init_openai_client()
    if client is None:
        return [None] * len(texts)

    result: List[Optional[bytes]] = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch = [t[:8000] if isinstance(t, str) and t.strip() else " " for t in batch]
        try:
            resp = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=batch
            )
            for item in resp.data:
                emb = item.embedding
                result.append(struct.pack(f"{len(emb)}f", *emb))
        except Exception as e:
            print(f"  âš ï¸ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            result.extend([None] * len(batch))
    return result


def normalize_item_keys(data_type: str, item: Dict[str, Any]) -> Dict[str, Any]:
    """
    alias ì²˜ë¦¬ + canonical key ì •ê·œí™” + íƒ€ì… ì •ë¦¬
    """
    schema = get_schema(data_type)
    aliases = schema.get("aliases", {})
    fields = schema["fields"]

    # 1) alias ë§¤í•‘
    tmp: Dict[str, Any] = {}
    for k, v in item.items():
        ck = aliases.get(k, k)
        tmp[ck] = v

    # 2) name field ë³´ì • (íƒ€ íƒ€ì… ì´ë¦„í•„ë“œê°€ ë“¤ì–´ì˜¨ ê²½ìš°)
    name_field = schema["name_field"]
    alt_name_fields = ["ê³¼ì œëª…", "ì§€í‘œëª…", "ì„¸ë¶€ì¶”ì§„ê³¼ì œëª…"]
    if name_field not in tmp:
        for nk in alt_name_fields:
            if nk in tmp and tmp[nk]:
                tmp[name_field] = tmp[nk]
                break

    # 3) canonical í•„ë“œë§Œ ìœ ì§€ + íƒ€ì… ì •ë¦¬
    out: Dict[str, Any] = {}
    for fname, spec in fields.items():
        t = spec["type"]
        val = tmp.get(fname, None)

        if t == "array":
            if val is None:
                out[fname] = []
            elif isinstance(val, list):
                out[fname] = [str(x).strip() for x in val if str(x).strip()]
            elif isinstance(val, str):
                s = val.strip()
                out[fname] = [s] if s else []
            else:
                out[fname] = [str(val).strip()] if str(val).strip() else []
        elif t == "object":
            if isinstance(val, dict):
                out[fname] = val
            else:
                out[fname] = {}
        else:  # string
            out[fname] = "" if val is None else str(val).strip()

    return out


def serialize_for_db(spec_type: str, value: Any) -> Any:
    """
    DB ì €ì¥ ì „ ì§ë ¬í™”:
    - array/object -> JSON ë¬¸ìì—´
    - string -> ë¬¸ìì—´
    """
    if spec_type == "array":
        if value is None:
            return json.dumps([], ensure_ascii=False)
        if isinstance(value, list):
            return json.dumps(value, ensure_ascii=False)
        if isinstance(value, str):
            s = value.strip()
            return json.dumps([s] if s else [], ensure_ascii=False)
        return json.dumps([str(value)], ensure_ascii=False)

    if spec_type == "object":
        if isinstance(value, dict):
            return json.dumps(value, ensure_ascii=False)
        return json.dumps({}, ensure_ascii=False)

    # string
    return "" if value is None else str(value)


def build_embedding_chunks(data_type: str, normalized_item: Dict[str, Any]) -> List[Dict[str, str]]:
    schema = get_schema(data_type)
    chunks: List[Dict[str, str]] = []

    for fname, spec in schema["fields"].items():
        if not spec.get("extract_detail", False):
            continue
        v = normalized_item.get(fname)

        if isinstance(v, list):
            for x in v:
                s = str(x).strip()
                if s:
                    chunks.append({"field_type": fname, "chunk_text": s})
        elif isinstance(v, str):
            s = v.strip()
            if s:
                chunks.append({"field_type": fname, "chunk_text": s})
        elif v is not None:
            s = str(v).strip()
            if s:
                chunks.append({"field_type": fname, "chunk_text": s})

    return chunks


# =========================================================
# í…Œì´ë¸” ìƒì„±/ë¦¬ì…‹ (ë™ì )
# =========================================================
def create_tables(conn):
    with conn.cursor() as cursor:
        # ë ˆê±°ì‹œ
        cursor.execute("DROP TABLE IF EXISTS embedding_chunks")

        # íƒ€ì…ë³„ ì„ë² ë”©/ë©”ì¸ í…Œì´ë¸” ë“œë¡­
        for dt, meta in SCHEMA_REGISTRY.items():
            cursor.execute(f"DROP TABLE IF EXISTS `{meta['embedding_table']}`")
        for dt, meta in SCHEMA_REGISTRY.items():
            cursor.execute(f"DROP TABLE IF EXISTS `{meta['table']}`")

        # ë©”ì¸ í…Œì´ë¸” ìƒì„±
        for dt, meta in SCHEMA_REGISTRY.items():
            col_defs = ["id INT AUTO_INCREMENT PRIMARY KEY"]
            for fname, spec in meta["fields"].items():
                db_t = spec["db"]
                required = "NOT NULL" if spec.get("required", False) else "NULL"
                col_defs.append(f"`{fname}` {db_t} {required}")

            col_defs.extend([
                "source_document VARCHAR(255)",
                "page_range VARCHAR(50)",
                "extraction_date VARCHAR(50)",
                "created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP"
            ])

            ddl = f"""
                CREATE TABLE `{meta['table']}` (
                    {", ".join(col_defs)}
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            cursor.execute(ddl)

        # ì„ë² ë”© í…Œì´ë¸” ìƒì„±
        for dt, meta in SCHEMA_REGISTRY.items():
            cursor.execute(f"""
                CREATE TABLE `{meta['embedding_table']}` (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    source_id INT NOT NULL,
                    item_name VARCHAR(500) NOT NULL,
                    field_type VARCHAR(100) NOT NULL,
                    chunk_text TEXT NOT NULL,
                    embedding BLOB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_source_id (source_id),
                    INDEX idx_item_name (item_name(255)),
                    INDEX idx_field_type (field_type)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """)

    conn.commit()
    print("âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ (schema-first)")


def reset_tables(conn):
    with conn.cursor() as cursor:
        for _, meta in SCHEMA_REGISTRY.items():
            cursor.execute(f"TRUNCATE TABLE `{meta['embedding_table']}`")
        for _, meta in SCHEMA_REGISTRY.items():
            cursor.execute(f"TRUNCATE TABLE `{meta['table']}`")
    conn.commit()
    print("ğŸ—‘ï¸ ëª¨ë“  í…Œì´ë¸” ë°ì´í„° ì‚­ì œ ì™„ë£Œ")


# =========================================================
# ë¡œë”© (ì™„ì „ ë™ì )
# =========================================================
def load_json_file(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {path}")
        return []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        print(f"âš ï¸ JSON ë£¨íŠ¸ê°€ listê°€ ì•„ë‹˜: {path}")
        return []
    return data


def add_embedding_chunks(cursor, data_type: str, source_id: int, item_name: str, chunks: List[Dict[str, str]]) -> int:
    if not item_name or not chunks:
        return 0

    schema = get_schema(data_type)
    emb_table = schema["embedding_table"]

    texts = [c["chunk_text"] for c in chunks]
    embeddings = get_embeddings_batch(texts)

    for c, emb in zip(chunks, embeddings):
        cursor.execute(f"""
            INSERT INTO `{emb_table}` (source_id, item_name, field_type, chunk_text, embedding)
            VALUES (%s, %s, %s, %s, %s)
        """, (
            source_id,
            item_name,
            c["field_type"],
            c["chunk_text"],
            emb
        ))

    return len(chunks)


def load_by_type(conn, data_type: str) -> Tuple[int, int]:
    """
    return: (loaded_count, chunk_count)
    """
    schema = get_schema(data_type)
    data = load_json_file(schema["json_path"])
    if not data:
        print(f"âš ï¸ {data_type} ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        return 0, 0

    table = schema["table"]
    name_field = schema["name_field"]
    fields = schema["fields"]

    loaded = 0
    total_chunks = 0

    with conn.cursor() as cursor:
        for raw_item in data:
            try:
                item = normalize_item_keys(data_type, raw_item)

                # ë©”ì¸ INSERT ë™ì  ìƒì„±
                field_names = list(fields.keys())
                db_columns = ", ".join([f"`{c}`" for c in field_names] + ["source_document", "page_range", "extraction_date"])
                placeholders = ", ".join(["%s"] * (len(field_names) + 3))

                field_values = [
                    serialize_for_db(fields[c]["type"], item.get(c))
                    for c in field_names
                ]

                source_document = raw_item.get("source_document", schema["json_path"])
                page_range = raw_item.get("page_range", "")
                extraction_date = raw_item.get("extraction_date", datetime.now().isoformat())

                cursor.execute(
                    f"INSERT INTO `{table}` ({db_columns}) VALUES ({placeholders})",
                    field_values + [source_document, page_range, extraction_date]
                )
                source_id = cursor.lastrowid

                # ì„ë² ë”© ì²­í¬
                chunks = build_embedding_chunks(data_type, item)
                chunks_added = add_embedding_chunks(
                    cursor=cursor,
                    data_type=data_type,
                    source_id=source_id,
                    item_name=item.get(name_field, ""),
                    chunks=chunks
                )
                total_chunks += chunks_added
                loaded += 1

            except Exception as e:
                title = raw_item.get(name_field) or raw_item.get("ê³¼ì œëª…") or raw_item.get("ì§€í‘œëª…") or raw_item.get("ì„¸ë¶€ì¶”ì§„ê³¼ì œëª…") or "Unknown"
                print(f"  âŒ ì˜¤ë¥˜: {str(title)[:30]} - {e}")

    conn.commit()
    return loaded, total_chunks


# =========================================================
# í†µê³„
# =========================================================
def show_stats(conn):
    print()
    print("=" * 70)
    print("ğŸ“Š DB í˜„í™©")
    print("=" * 70)

    total_items = 0
    total_chunks = 0

    with conn.cursor() as cursor:
        for dt, meta in SCHEMA_REGISTRY.items():
            cursor.execute(f"SELECT COUNT(*) AS cnt FROM `{meta['table']}`")
            item_cnt = cursor.fetchone()["cnt"]

            cursor.execute(f"SELECT COUNT(*) AS cnt FROM `{meta['embedding_table']}`")
            chunk_cnt = cursor.fetchone()["cnt"]

            total_items += item_cnt
            total_chunks += chunk_cnt

            label = meta.get("type_display", dt)
            print(f"  {label:<12}: {item_cnt:>5}ê°œ (ì„ë² ë”© ì²­í¬: {chunk_cnt}ê°œ)")

    print(f"  {'â”€' * 55}")
    print(f"  ì´ í•­ëª©      : {total_items:>5}ê°œ")
    print(f"  ì´ ì„ë² ë”©ì²­í¬: {total_chunks:>5}ê°œ")
    print("=" * 70)


# =========================================================
# CLI
# =========================================================
def main():
    parser = argparse.ArgumentParser(
        description="JSON íŒŒì¼ì„ MariaDB/MySQLì— ë¡œë“œ (schema-first)"
    )

    parser.add_argument(
        "--type", "-t",
        choices=["all"] + list(SCHEMA_REGISTRY.keys()),
        default="all",
        help="ë¡œë“œí•  ë°ì´í„° íƒ€ì…"
    )
    parser.add_argument("--reset", "-r", action="store_true", help="ê¸°ì¡´ ë°ì´í„° TRUNCATE í›„ ë¡œë“œ")
    parser.add_argument("--db-host", default=DEFAULT_DB_HOST, help="DB í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--db-port", type=int, default=DEFAULT_DB_PORT, help="DB í¬íŠ¸")
    parser.add_argument("--db-name", default=DEFAULT_DB_NAME, help="DB ì´ë¦„")
    parser.add_argument("--db-user", default=DEFAULT_DB_USER, help="DB ì‚¬ìš©ì")
    parser.add_argument("--db-password", default=DEFAULT_DB_PASSWORD, help="DB ë¹„ë°€ë²ˆí˜¸")

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸš€ JSON â†’ MariaDB ë¡œë“œ ì‹œì‘ (schema-first)")
    print("=" * 60)
    print(f"  DB: {args.db_user}@{args.db_host}:{args.db_port}/{args.db_name}")
    print(f"  íƒ€ì…: {args.type}")
    print(f"  ì´ˆê¸°í™”: {'ì˜ˆ' if args.reset else 'ì•„ë‹ˆì˜¤'}")
    print("=" * 60)

    try:
        conn = get_connection(
            host=args.db_host,
            port=args.db_port,
            database=args.db_name,
            user=args.db_user,
            password=args.db_password,
        )
        print("âœ… DB ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

    try:
        # í•­ìƒ registry ê¸°ì¤€ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±
        create_tables(conn)

        if args.reset:
            reset_tables(conn)

        print()
        total_loaded = 0
        total_chunks = 0

        targets = list(SCHEMA_REGISTRY.keys()) if args.type == "all" else [args.type]

        for dt in targets:
            meta = get_schema(dt)
            print(f"ğŸ“¥ {meta.get('type_display', dt)} ë¡œë“œ ì¤‘... ({meta['json_path']})")
            loaded, chunks = load_by_type(conn, dt)
            print(f"   â†’ {loaded}ê°œ ë¡œë“œ ì™„ë£Œ / ì„ë² ë”© ì²­í¬ {chunks}ê°œ")
            total_loaded += loaded
            total_chunks += chunks

        print()
        print(f"âœ… ì´ {total_loaded}ê°œ í•­ëª© ë¡œë“œ ì™„ë£Œ")
        print(f"âœ… ì´ {total_chunks}ê°œ ì„ë² ë”© ì²­í¬ ìƒì„± ì™„ë£Œ")

        show_stats(conn)

    finally:
        conn.close()


if __name__ == "__main__":
    main()
